# 虚拟机准备步骤

1. 安装最小安装的虚拟机

2. vi /etc/sysconfig/network-scripts/ifcfg-ens33，将onboot改成yes

3. 重启network服务

   ```bash
   systemctl restart network
   ```

4. 查看IP地址

   ```bash
   ip addr show
   ```

5. 用shell工具连接虚拟机

6. 安装必要软件包

   ```bash
   yum install -y epel-release
   yum install -y psmisc nc net-tools rsync vim lrzsz ntp libzstd openssl-static tree iotop git
   ```

7. 创建新用户（atguigu）

   ```bash
   useradd atguigu
   ```

8. 修改atguigu密码

   ```bash
   passwd atguigu
   ```

9. visudo给atguigu添加超级权限

   ```bash
   visudo
   ```

   ```
   添加一行：
   atguigu  ALL=(ALL)     NOPASSWD:ALL
   ```

10. 关闭防火墙

    ```bash
    systemctl stop firewalld
    systemctl disable firewalld
    ```

11. 创建两个目录并修改所有者和所属组

    ```bash
    mkdir /opt/module /opt/software
    chown -R atguigu:atguigu /opt/module /opt/software
    ```

12. 修改本机hostname

    ```bash
    hostnamectl --static set-hostname hadoop101
    ```

13. 修改本机静态IP

    ```bash
    vim /etc/sysconfig/network-scripts/ifcfg-ens33
    ```

    ```ini
    TYPE=Ethernet
    BOOTPROTO=static
    NAME=ens33
    DEVICE=ens33
    ONBOOT=yes
    IPADDR=192.168.5.102
    UUID=14a6ebe4-ae2e-44b3-bb14-1340cdc82a00
    PREFIX=24
    GATEWAY=192.168.5.2
    DNS1=192.168.5.2
    ```

14. 修改/etc/hosts文件，和本机hostname和静态IP对应

    ```bash
    for ((i=100;i<110;i++))
    do
    	echo "192.168.5.$i hadoop$i" >> /etc/hosts
    done
    ```

15. 关机，快照，克隆

# 在102部署JDK和Hadoop环境

1. 将JDK和Hadoop的tar包拷贝到/opt/software

2. 解压

   ```bash
   cd /opt/software
   ls *.tar.gz | xargs -n1 tar -zxC /opt/module -f
   ```

3. 配置环境变量

   ```bash
   sudo vim /etc/profile.d/my_env.sh
   ```

   ```bash
   #JAVA_HOME
   export JAVA_HOME=/opt/module/jdk1.8.0_212
   export PATH=$PATH:$JAVA_HOME/bin
   #HADOOP_HOME
   export HADOOP_HOME=/opt/module/hadoop-3.1.3
   export PATH=$PATH:$HADOOP_HOME/bin
   export PATH=$PATH:$HADOOP_HOME/sbin
   ```

   重启ssh环境

# 配置远程同步脚本xsync

```bash
cd
vim xsync
```

```bash
#!/bin/bash
#1. 判断参数个数
if [ $# -lt 1 ]
then
  echo Not Enough Arguement!
  exit;
fi
#2. 遍历集群所有机器
for host in hadoop102 hadoop103 hadoop104
do
  echo ====================  $host  ====================
  #3. 遍历所有目录，挨个发送
  for file in $@
  do
    #4. 判断文件是否存在
    if [ -e $file ]
    then
      #5. 获取父目录
      pdir=$(cd -P $(dirname $file); pwd)
      #6. 获取当前文件的名称
      fname=$(basename $file)
      ssh $host "mkdir -p $pdir"
      rsync -av $pdir/$fname $host:$pdir
    else
      echo $file does not exists!
    fi
  done
done
```

```bash
chmod +x xsync
sudo cp xsync /bin
rm xsync
```

# 免密登陆配置

在102上执行

```bash
ssh-keygen -t ecdsa
ssh-copy-id hadoop102
xsync ~/.ssh
```

# 集群配置

在102上改文件

core-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://hadoop102:9820</value>
</property>
<!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/opt/module/hadoop-3.1.3/data</value>
</property>

<!-- 配置HDFS网页登录使用的静态用户为atguigu -->
    <property>
        <name>hadoop.http.staticuser.user</name>
        <value>atguigu</value>
</property>

<!-- 配置该atguigu(superUser)允许通过代理访问的主机节点 -->
    <property>
        <name>hadoop.proxyuser.atguigu.hosts</name>
        <value>*</value>
</property>
<!-- 配置该atguigu(superUser)允许通过代理用户所属组 -->
    <property>
        <name>hadoop.proxyuser.atguigu.groups</name>
        <value>*</value>
</property>
</configuration>
```

hdfs-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- nn web端访问地址-->
	<property>
        <name>dfs.namenode.http-address</name>
        <value>hadoop102:9870</value>
    </property>
	<!-- 2nn web端访问地址-->
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>hadoop104:9868</value>
    </property>
</configuration>

```

yarn-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
</property>
<!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>hadoop103</value>
</property>
<!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
</property>
<!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>512</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>4096</value>
</property>
<!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>4096</value>
</property>
<!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
</configuration>

```

mapred-site.xml

```xml
<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
	<!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
</configuration>

```

workers

```ini
hadoop102
hadoop103
hadoop104
```

同步配置文件

```bash
xsync /opt/module/hadoop-3.1.3/etc
```

格式化nn，在102执行

```bash
hdfs namenode -format
```

启动集群

102

```bash
start-dfs.sh
```

103

```bash
start-yarn.sh
```

查看网址：hadoop102:9870  hadoop103:8088

# 测试集群

上传一个输入文件

输入测试命令

```bash
[atguigu@hadoop102 hadoop-3.1.3]$ hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.1.3.jar wordcount /wcinput /output
```

# 翻车了怎么办

1. 停止集群所有进程

   102

   ```
   stop-dfs.sh
   ```

   103

   ```
   stop-yarn.sh
   ```

   或者在2，3，4上同时执行

   ```
   killall -9 java
   ```

   在3台节点上删除数据文件

   ```
   rm -rf /opt/module/hadoop-3.1.3/data /opt/module/hadoop-3.1.3/logs
   ```

   从格式化开始重新来